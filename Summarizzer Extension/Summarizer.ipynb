{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tricks\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tricks\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/10.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/10.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.0 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.0 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/10.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'c:\\Users\\Tricks\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'c:\\Users\\Tricks\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\tricks\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.6 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/2.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/3.1 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.6/3.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.4/3.1 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed cffi-1.17.1 cryptography-43.0.3 pdfminer.six-20231228 pdfplumber-0.11.4 pycparser-2.22 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pypdfium2.exe is installed in 'c:\\Users\\Tricks\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'c:\\Users\\Tricks\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BartTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pdfplumber\n",
    "\n",
    "def summarize_text(text, max_length=150, min_length=50, do_sample=False):\n",
    "    \"\"\"\n",
    "    Summarizes the input text using a pre-trained Hugging Face transformer model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to summarize.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "        do_sample (bool): Whether or not to sample the output.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the text.\n",
    "    \"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=do_sample)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "def chunk_text_with_tokenizer(text, tokenizer, max_tokens=450):\n",
    "    \"\"\"\n",
    "    Splits text into smaller chunks using a tokenizer to ensure token limits.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be split.\n",
    "        tokenizer: The Hugging Face tokenizer for the summarization model.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        generator: A generator yielding text chunks.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False).input_ids[0]\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    for chunk in chunks:\n",
    "        yield tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "\n",
    "# Example usage with BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "\n",
    "def summarize_text_in_chunks(text, max_length=150, min_length=50):\n",
    "    \"\"\"\n",
    "    Summarizes large text by splitting it into chunks and summarizing each chunk.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The combined summary of all chunks.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    for chunk in chunk_text_with_tokenizer(text, tokenizer, max_tokens=450):\n",
    "        try:\n",
    "            summary = summarize_text(chunk, max_length=max_length, min_length=min_length)\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing chunk: {e}\")\n",
    "            print(f\"Problematic chunk: {chunk[:200]}...\")  # Log the first 200 characters\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "\n",
    "def summarize_document(file_path, max_length=150, min_length=50):\n",
    "    \"\"\"\n",
    "    Reads a document and summarizes its content.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document file.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the document content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "        return summarize_text_in_chunks(text, max_length=max_length, min_length=min_length)\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {e}\"\n",
    "\n",
    "def summarize_website(url, max_length=150, min_length=50):\n",
    "    \"\"\"\n",
    "    Fetches content from a website and summarizes it.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to summarize.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the website content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extracting text content from the website\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([p.get_text() for p in paragraphs])\n",
    "\n",
    "        return summarize_text_in_chunks(content, max_length=max_length, min_length=min_length)\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching website: {e}\"\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using pdfplumber, handling tables and empty pages.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            if not pdf.pages:\n",
    "                return \"No pages found in the PDF.\"\n",
    "\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    elif page.extract_tables():\n",
    "                        # Extract tables if no text is present\n",
    "                        tables = page.extract_tables()\n",
    "                        for table in tables:\n",
    "                            table_text = \"\\n\".join([\"\\t\".join(row) for row in table if row])\n",
    "                            text += table_text + \"\\n\"\n",
    "                        print(f\"Extracted table data from page {i + 1}.\")\n",
    "                    else:\n",
    "                        print(f\"Warning: No text or table found on page {i + 1}.\")\n",
    "                except Exception as page_error:\n",
    "                    print(f\"Error processing page {i + 1}: {page_error}\")\n",
    "        \n",
    "        return text if text.strip() else \"No readable text found in the PDF.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from PDF: {e}\"\n",
    "\n",
    "\n",
    "def summarize_pdf(file_path, max_length=150, min_length=50):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and summarizes its content.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "        max_length (int): Maximum length of the summary.\n",
    "        min_length (int): Minimum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the PDF content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text using pdfplumber\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text.strip() or text == \"No text found in the PDF.\":\n",
    "            return \"No readable text found in the PDF.\"\n",
    "        \n",
    "        # Summarize the text in chunks\n",
    "        return summarize_text_in_chunks(text, max_length=max_length, min_length=min_length)\n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDF file: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Last Light is about a lighthouse in a forgotten village. The villagers had long stopped sailing, and the beacon atop the tower hadn't lit in decades. Yet every day, an old man would climb the narrow spiral stairs to the top. \"The light isn’t for the sea, child,\" he said, \"It’s for the ones who are still looking for home\"\n"
     ]
    }
   ],
   "source": [
    "print(summarize_pdf(\"The Last Light.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In threaded programming, pthread_join() is used for pausing the main thread until another thread completes/terminates. A process control block (PCB) is a data structure maintained by the operating system for every active process. The program counter plays a vital role in ensuring that processes resume correctly after an interrupt. Calculate the Average waiting time and average turnaround time using the following algorithm:. Shortest Job Remaining Time Next. Highest Response Ratio Next. Average Turnaround Time = 11.5. Average Waiting Time = 5.75. An embedded system is a computer system that is part of a larger device or system. It typically has a dedicated function and operates in real-time environments. Deeply embedded systems are designed to perform very specific, low-level tasks. These systems are typically invisible and fully integrated. Creating processes using the fork() system call in a certain way, will result in a hierarchy of processes. Consider the following hierarchy:Write a C program to create such a hierarchy. Each child simply display its process id.  complex tasks with external interfaces, while deeply embedded systems are focused on specific, low-level tasks.\n"
     ]
    }
   ],
   "source": [
    "print(summarize_pdf(\"Sample.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structures and Algorithms (DSA) refer to the study of methods for organizing and storing data. DSA is one of the most important skills that every computer science student must have. Top companies like Google, Microsoft, Amazon, Apple, Meta and many other companies heavily focus on data structures and algorithms during interviews.  Mathematical algorithms are a class of algorithms that solve problems related to mathematical concepts. They are widely used in various fields, including Computer graphics, Numerical analysis, Optimization and Cryptography. Asymptotic notation is a mathematical tool that estimates time based on input size without running the code. Recursion is a programming technique where a function calls itself within its own definition. It is usually used to solve problems that can be broken down into smaller instances of the same problem. Recursion forms the base for many other algorithms such as Tree traversals, Graph traversal, Divide and Conquers Algorithms and Backtracking algorithms. A tree is a non-linear hierarchical data structure consisting of nodes connected by edges, with a top node called the root and nodes having child nodes. It is used in computer science for organizing data efficiently. A Heap is a complete binary tree data structure that satisfies the heap property: for every node, the value of its children is less than or equal to its own value. Graph algorithms in data structures and algorithms (DSA) are a set of techniques and methods used to solve problems related to graphs. These algorithms are designed to perform various operations on graphs. They are essential for solving a wide range of real-world problems, including network routing, social network analysis, and resource allocation.  Pattern searching is a fundamental technique in DSA used to find occurrences of a specific pattern within a given text. The Branch and Bound Algorithm is a method used in combinatorial optimization problems to systematically search for the best solution. Geometric algorithms are a class of algorithms that solve problems related to geometry.\n"
     ]
    }
   ],
   "source": [
    "print(summarize_website(\"https://www.geeksforgeeks.org/learn-data-structures-and-algorithms-dsa-tutorial/?ref=home-articlecards\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
